{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import jsbeautifier\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recorders = ['salman', 'tarab', 'negin', 'fada', 'rajab']\n",
    "\n",
    "data_dir = '/Users/bobby/Desktop/Projects/Awesome-Fun-And-Glorious-Gloves-Of-Sign-Language/data/data with new glove'\n",
    "\n",
    "features_index = {\n",
    "  # \"quat_w\": 0,\n",
    "  # \"quat_x\": 1,\n",
    "  # \"quat_y\": 2,\n",
    "  # \"quat_z\": 3,\n",
    "  # \"euler_x\": 4,\n",
    "  # \"euler_y\": 5,\n",
    "  # \"euler_z\": 6,\n",
    "  \"acc_x\": 7,\n",
    "  \"acc_y\": 8,\n",
    "  \"acc_z\": 9,\n",
    "  \"line_acc_x\": 10,\n",
    "  \"line_acc_y\": 11,\n",
    "  \"line_acc_z\": 12,\n",
    "  \"gyro_x\": 13,\n",
    "  \"gyro_y\": 14,\n",
    "  \"gyro_z\": 15,\n",
    "  \"gravity_x\": 16,\n",
    "  \"gravity_y\": 17,\n",
    "  \"gravity_z\": 18,\n",
    "  \"flex_0\": 19,\n",
    "  \"flex_1\": 20,\n",
    "  \"flex_2\": 21,\n",
    "  \"flex_3\": 22,\n",
    "  \"flex_4\": 23\n",
    "}\n",
    "\n",
    "n_features = len(features_index.keys())\n",
    "\n",
    "index_feature = {value:key for key,value in features_index.items()}\n",
    "\n",
    "features = np.array([features_index[key] for key in features_index])\n",
    "\n",
    "labels_map = {\n",
    "  \"Abi\": 0,\n",
    "  \"Sabz\": 1,\n",
    "  \"Saal\": 2,\n",
    "  \"Ruz\": 3,\n",
    "  \"Faramush\": 4,\n",
    "  \"Ast\": 5,\n",
    "  \"Kheili\": 6,\n",
    "  \"Tabestun\": 7,\n",
    "  \"Bakht\": 8,\n",
    "  \"Diruz\": 9,\n",
    "  \"Omidvar\": 10,\n",
    "  \"Maman\": 11,\n",
    "  \"Baba\": 12,\n",
    "  \"Khosh\": 13,\n",
    "  \"Like\": 14,\n",
    "  \"Dislike\": 15\n",
    "}\n",
    "\n",
    "n_clusters = 16\n",
    "\n",
    "index_label = {value:key for key,value in labels_map.items()}\n",
    "\n",
    "# printing\n",
    "options = jsbeautifier.default_options()\n",
    "options.indent_size = 2\n",
    "\n",
    "#pie chart colors\n",
    "random.seed(42)\n",
    "colors = {key: (random.random(), random.random(), random.random()) for key in labels_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implement a function to exract the label given a file's name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_file_label(filename: str) -> List[int]:\n",
    "  filename = filename.split('/')[-1].split('.')[0]\n",
    "  labels = re.findall('[A-Z][^A-Z]*', filename)\n",
    "  return [labels_map[label] for label in labels]\n",
    "\n",
    "extract_file_label('KheiliKheiliAbi.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need  a function to read files and extract time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line2float(line: str, features: np.array = None):\n",
    "  if features is None:\n",
    "      return np.array([float(i) for i in line[:-2].split(\" \")])\n",
    "  return np.array([float(i) for i in line[:-2].split(\" \")])[features]\n",
    "\n",
    "def read_file(file:str, features: np.array = None):\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    x = [[]]\n",
    "    for i in range(len(lines)):\n",
    "        if \";\" in lines[i]:\n",
    "            x.append([])\n",
    "        else:\n",
    "            x[-1].append(line2float(lines[i], features=features))\n",
    "    if x[-1] == []:\n",
    "        return x[:-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salman 616\n",
      "tarab 807\n",
      "negin 355\n",
      "fada 382\n",
      "rajab 404\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_dir: str, recorders: List[str], features: np.array = None):\n",
    "  dataset = {}\n",
    "  for recorder in recorders:\n",
    "    files = os.listdir(f'{data_dir}/{recorder}')\n",
    "    if not recorder in dataset:\n",
    "      dataset[recorder] = []\n",
    "    for f in files:\n",
    "      data = read_file(f'{data_dir}/{recorder}/{f}', features=features)\n",
    "      label = extract_file_label(f)\n",
    "      for sample in data:\n",
    "        dataset[recorder].append((np.array(sample), label))\n",
    "  return dataset\n",
    "\n",
    "dataset = load_dataset(data_dir, all_recorders, features=features)\n",
    "for recorder in all_recorders:\n",
    "  print(recorder, len(dataset[recorder]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are some outlier data that may cause problem in our normalization process:\n",
    "- quat_y > 1.5\n",
    "- euler_x > 500 or < -500\n",
    "- acc_x > 100 or < -100\n",
    "- acc_z > 100 or < -100\n",
    "- gyro_z > 20\n",
    "- gravity_y > 11\n",
    "\n",
    "Lets see to what sample does these outliers belong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: salman 267 acc_z -320.45 [9, 1]\n",
      "Bigger: salman 280 gyro_z 36.01 [8, 4]\n",
      "Bigger: salman 366 gravity_y 38.9 [5, 1]\n",
      "Bigger: salman 475 acc_x 163.71 [3, 0, 2]\n",
      "Bigger: tarab 577 acc_z 322.91 [6, 8, 1]\n",
      "Smaller: tarab 654 acc_x -322.89 [10, 4]\n",
      "{'salman': [267, 280, 366, 475], 'tarab': [577, 654]}\n"
     ]
    }
   ],
   "source": [
    "# outlier_bt = {'quat_y': 1.5, 'euler_x': 500, 'acc_x': 100, 'acc_z': 100, 'gyro_z': 20, 'gravity_y': 11}\n",
    "# outlier_bt = {'euler_x': 500, 'acc_x': 100, 'acc_z': 100, 'gyro_z': 20, 'gravity_y': 11}\n",
    "# outlier_lt = {'euler_x': -500, 'acc_x': -100, 'acc_z': -100}\n",
    "\n",
    "# outlier_bt = {'gyro_z': 20, 'gravity_y': 11}\n",
    "\n",
    "outlier_bt = {'acc_x': 100, 'acc_z': 100, 'gyro_z': 20, 'gravity_y': 11}\n",
    "outlier_lt = {'acc_x': -100, 'acc_z': -100}\n",
    "\n",
    "index_relative_feature = {key : i for i, key in enumerate(features_index.keys())}\n",
    "\n",
    "outlier_ids = dict()\n",
    "\n",
    "for recorder in all_recorders:\n",
    "  for id, sample_label_pair in enumerate(dataset[recorder]):\n",
    "      sample, label = sample_label_pair\n",
    "      for feature in outlier_bt:\n",
    "         index = index_relative_feature[feature]\n",
    "         for i in range(sample.shape[0]):\n",
    "            if sample[i, index] > outlier_bt[feature]:\n",
    "               print('Bigger:', recorder, id, feature, sample[i, index], label)\n",
    "               if recorder not in outlier_ids:\n",
    "                  outlier_ids[recorder] = []\n",
    "               outlier_ids[recorder].append(id)\n",
    "               break\n",
    "      \n",
    "      for feature in outlier_lt:\n",
    "         index = index_relative_feature[feature]\n",
    "         for i in range(sample.shape[0]):\n",
    "            if sample[i, index] < outlier_lt[feature]:\n",
    "               print('Smaller:', recorder, id, feature, sample[i, index], label)\n",
    "               if recorder not in outlier_ids:\n",
    "                  outlier_ids[recorder] = []\n",
    "               outlier_ids[recorder].append(id)\n",
    "               break\n",
    "\n",
    "print(outlier_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets remove those outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salman 612\n",
      "tarab 805\n",
      "negin 355\n",
      "fada 382\n",
      "rajab 404\n"
     ]
    }
   ],
   "source": [
    "def delete_items_from_list(the_list: list, indexes: List[int]):\n",
    "  for index in sorted(indexes, reverse=True):\n",
    "        del the_list[index]\n",
    "      \n",
    "for recorder, ids in outlier_ids.items():\n",
    "      delete_items_from_list(dataset[recorder], ids)\n",
    "          \n",
    "for recorder in all_recorders:\n",
    "  print(recorder, len(dataset[recorder]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the minimum and maximum range of each feature to use it our normalization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-39.2    -26.23   -38.68   -39.4    -17.21   -34.64   -13.3511 -28.8367\n",
      "  -11.6756  -9.35    -9.8     -9.8      0.       5.     123.       5.\n",
      "   97.    ]]\n",
      "[[ 39.41    39.97    39.87    30.03    30.95    43.13    16.2378  29.8933\n",
      "   19.7389   9.8      9.18     9.8    405.     613.     493.     568.\n",
      "  380.    ]]\n"
     ]
    }
   ],
   "source": [
    "mins = np.ones((1, n_features), dtype=float) * (10 ** 6)\n",
    "maxs = np.ones((1, n_features), dtype=float) * (-10 ** 5)\n",
    "\n",
    "for _ , samples in dataset.items():\n",
    "  for sample_label_pair in samples:\n",
    "    sample, _ = sample_label_pair\n",
    "    data_mins = np.min(sample, axis = 0, keepdims = True)\n",
    "    data_maxs = np.max(sample, axis = 0, keepdims = True)\n",
    "    mins = np.minimum(mins, data_mins)\n",
    "    maxs = np.maximum(maxs, data_maxs)\n",
    "\n",
    "print(mins)\n",
    "print(maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Nearest Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we implement a function to filter the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset: Dict[str, List[Tuple[np.array, np.array]]], recorders: List[str],\n",
    "                    max_word_length: int, min_word_length: int  = 1):\n",
    "  labels = []\n",
    "  series = []\n",
    "  series_recorders = []\n",
    "  for recorder in recorders:\n",
    "    for sample_label_pair in dataset[recorder]:\n",
    "        sample, label = sample_label_pair\n",
    "        if min_word_length <= len(label) <= max_word_length:\n",
    "          sample_np = (np.array(sample) - mins) / (maxs - mins)\n",
    "          series.append(sample_np.tolist())\n",
    "          labels.append(label)\n",
    "          series_recorders.append(recorder)\n",
    "\n",
    "  return series, labels, series_recorders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement our distance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dtw(series1, series2, r1=3, r2=3):\n",
    "\n",
    "    def euclidean_dist(sample1, sample2):\n",
    "        return np.sum(np.power(np.array(sample1) - np.array(sample2), 2))\n",
    "    \n",
    "    # distances = np.zeros((len(series1) + 1, len(series2) + 1), dtype=np.float64)\n",
    "    distances = np.ones((len(series1) + 1, len(series2) + 1), dtype=np.float64) * np.inf\n",
    "    directions = np.zeros((len(series1) + 1, len(series2) + 1), dtype=np.int8)\n",
    "\n",
    "    for i in range(r1 + 1):\n",
    "        distances[i, 0] = 0\n",
    "        directions[i, 0] = 2\n",
    "    \n",
    "    for j in range(r2 + 1):\n",
    "        distances[0, j] = 0\n",
    "        directions[0, j] = 3\n",
    "\n",
    "    for i in range(1, distances.shape[0]):\n",
    "        for j in range(1, distances.shape[1]):\n",
    "            min_value = distances[i - 1, j - 1]\n",
    "            directions[i, j] = 1\n",
    "            if distances[i - 1, j] < min_value:\n",
    "                min_value = distances[i - 1, j]\n",
    "                directions[i, j] = 2\n",
    "            if distances[i, j - 1] < min_value:\n",
    "                min_value = distances[i, j - 1]\n",
    "                directions[i, j] = 3\n",
    "\n",
    "            distances[i, j] = min_value + euclidean_dist(series1[i - 1], series2[j - 1])\n",
    "\n",
    "    #p = (distances.shape[0] - 1, distances.shape[1] - 1)\n",
    "    min_row_value = float('inf')\n",
    "    min_col_value = float('inf')\n",
    "    min_row_point = None\n",
    "    min_col_point = None\n",
    "    n, m = distances.shape[0] - 1, distances.shape[1] - 1\n",
    "    for i in range(r1 + 1):\n",
    "        if distances[n - i, m] < min_col_value:\n",
    "            min_col_point = (n - i, m)\n",
    "            min_col_value = distances[n - i, m]\n",
    "\n",
    "    for j in range(r2 + 1):\n",
    "        if distances[n, m - j] < min_row_value:\n",
    "            min_row_point = (n, m - j)\n",
    "            min_row_value = distances[n, m - j]\n",
    "    \n",
    "    p = min_col_point if min_col_value < min_row_value else min_row_point\n",
    "    final_point = p\n",
    "    points = []\n",
    "    while p[0] > r1 or p[1] > r2:   \n",
    "        points.append((p[0] - 1, p[1] - 1))\n",
    "        if directions[p[0], p[1]] == 1:\n",
    "            p = (p[0] - 1, p[1] - 1)\n",
    "        elif directions[p[0], p[1]] == 2:\n",
    "            p = (p[0] - 1, p[1])\n",
    "        elif directions[p[0], p[1]] == 3:\n",
    "            p = (p[0], p[1] - 1)\n",
    "    points.append((p[0] - 1, p[1] - 1))\n",
    "    \n",
    "    points.reverse()\n",
    "    return distances[final_point[0], final_point[1]], distances[1:, 1:], points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here are our functions to calculate the nearest(s) samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearests(series, labels, sample, top=1):\n",
    "    '''Finds the top nearests samples to the given sample (including itself if exists in series)'''\n",
    "    nearests = []\n",
    "    for i, s in enumerate(series):\n",
    "        dist = my_dtw(series[i], sample)[0]\n",
    "        # dist = dtw_path(series[i], sample)[1]\n",
    "        nearests.append((i, dist, labels[i]))\n",
    "\n",
    "    nearests = sorted(nearests, reverse=False, key=lambda x: x[1])\n",
    "    return nearests[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(series, labels, sample, ignore_index=[]):\n",
    "    '''Finds the nearest sample to the given sample with index.'''\n",
    "    min_dist = float('inf')\n",
    "    min_index = index\n",
    "    min_label = labels[index]\n",
    "    for i, s in enumerate(series):\n",
    "        if i not in ignore_index:\n",
    "            dist = my_dtw(series[i], sample)[0]\n",
    "            # dist = dtw_path(series[i], sample)[1]\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_index = i\n",
    "                min_label = labels[i]\n",
    "    return min_index, min_dist, min_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Based on the data belonging to a specific recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets ge the data belonging to one user: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_recorders = ['salman']\n",
    "series, labels, series_recorders = filter_dataset(dataset, target_recorders, max_word_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901960784313726\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, s in enumerate(series):\n",
    "    top1 = find_nearest(series, labels, series[i], ignore_index=[i])\n",
    "    if top1[2] == labels[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct / len(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see the the data label-wise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Based on the data belonging to all recorders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose a test recorder and try to classify his/her samples using the data belonging to other recorders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recorder = 'salman'\n",
    "other_recorders = [recorder for recorder in all_recorders if recorder != test_recorder]\n",
    "\n",
    "test_series, test_labels, _ = filter_dataset(dataset, [test_recorder], max_word_length=1)\n",
    "other_series, other_labels, _ = filter_dataset(dataset, other_recorders, max_word_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the top-5 accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607843137254902\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, s in enumerate(test_series):\n",
    "    top5 = find_nearests(other_series, other_labels, s, top=5)\n",
    "    for result in top5:\n",
    "        if result[2] == test_labels[i]:\n",
    "            correct += 1\n",
    "            break\n",
    "\n",
    "print(correct / len(test_series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the top-1 accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803921568627451\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, s in enumerate(test_series):\n",
    "    top1 = find_nearest(other_series, other_labels, s)\n",
    "    if top1[2] == test_labels[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct / len(test_series))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
